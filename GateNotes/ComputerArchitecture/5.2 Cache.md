
- **What is it?**
	- Faster memory compared to ram
	- Contains blocks of data
		- Each block is the same size of the block present in main memory
		- The number of block in cache is less compared to the number of blocks in the main memory
		- The block in the cache is also called a **cache line**
- **How is data moved from main memory to cache?**
	- A block from main memory is moved to cache on demand
		- This is done by the mapping algorithm
- **What happens when the cache is full?**
	- When cache is full and another block is needed
		- The **replacement algorithm** moves a block back to main memory
- **Accessing the cache**
	- Processor initially send a logical address, this logical address is converted to the physical address using TLB and Page table 
		- Refer [[4.1 Memory Organization]] for logical address
		- Refer [[4.2 Paging]] for logical to physical address translation
	- Now the cache is searched with the computed physical address
	- Case 1: Cache hit
		- The contents of the memory location are already present in the cache
		- Read: no additional op required
		- Write
			- Write through: immediately update cache & main memory
			- Write back: have a dirty bit, write the changes to main memory when block is moved to out of cache  
	- Case 2: Cache miss:
		- Read miss
			- The entire block is moved from main memory to cache
		- Write miss
			- Write through: The write operation is performed on the main memory
			- Write back: The block is moved to the cache and write operation is performed

##### 1 **Mapping algorithms**
- Example case
	- Cache memory:
		- 128 blocks
		- Each block has 16 words
	- Main memory
		- 32 * 128 blocks = 4096 blocks
		- Each block has 16 words
	- Address
		- 16 bits

##### 1.1 **Direct mapping**
- Mapping function uses modulus
	- block 0 in main mem will be stored in (0 mod 128)th block in cache 
	- block 1 in main mem will be stored in (1 mod 128)th block in cache 
- Drawback
	- For the block 0 in cache, block 0, 128, 256, 512 ..... will be stored
	- Let say cache is empty, block 0 and block 128 is required
		- Even if the cache has free space, block 0 from cache will be moved into main mem to load block 128 to cache
- Addressing
	- 5 tag bits: 
		- Input: 32\*128 blocks, 5 bit address
		- Output: Select one 128 set of blocks   
		- Logically the main memory is divided into 2<sup>5</sup> parts, each part is 128 bits
		- Select a collection of 128 blocks form 32 * 128 blocks
	- 7 block bits
		- input: 128 blocks, 7 bits
		- output: one block form 128 blocks
	- 4 bits
		- input: 1 block, 4 bits
		- Output: 1 word from 16 words
- Processor read request
	- Process gives the 16 bit address
	- The 7 block bits are used to find the respective block in cache
	- The tag bits given by the processor is compared with the tag bits of block that are stored in the cache  
		- Tag bits does not match: Block is moved from main memory
	- Tag bits match: find the respective word using the last 4 bits

##### 1.2 Associative mapping
- Any block from main mem can be placed in any block in cache
- A block is replaced by replacement algo only if cache is full
- Addressing
	- 12 tag bits
		- to identify a block, 2^12 = 4096
	- 4 bits
		- identify a word from block
- Drawback
	- As any block in main mem can occupy any block in cache searching for a block takes liner time
		- In this case, 128 blocks are present in cache. Worst case all 128 blocks are searched to determine if block requested by processor is in cache 
	- The time can be reduced by parallel search, but this increases the cost of an associative cache 

##### 1.2 Set Associative mapping
- Combines direct and set associative mapping
- The cache has multiple sets
	- Each set is a collection of blocks in cache
	- A block in main mem can be present anywhere in a particular set
	- That is, each block is mapped to a set in cache
		- Each set can hold fixed number of blocks
		- if set size is 2, there will be 64 sets in cache
		- Each set will be associated to 4096 blocks / 64 sets
			- 64 blocks in main mem will be mapped to the same set 
- If the set size if 1
	- This becomes a direct mapping scheme
- If the set size is the size of cache 
	- This becomes a associative mapping scheme
- Addressing
	- 6 bits are tag bits
	- 6 bits for identifying a set in cache 
		- Cache has a set size of 2 -> 
		- Cache has 128 words / 2 works per set = 64 sets
		- To represent 64 sets, you'll need 6 bits (as using 6 bits there are 64 unique combinations) 
	- 4 bits for identifying a word from a block
- Searching for a word
	- CPU gives the 16 bit address
	- Use the second 6 set bits to determine the set
	- Use the first 6 bit tags to determine the block in the set
		- Need to parallelly compare this tag bits with two blocks in the set
		- Cache miss
			- 6 bits don't match with both block
		- Cache hit 
			- 6 bits match with one of the blocks in set
	- Use the 4 bit to determine the word from the block


#### 2 Total cache memory
- **Why store tag bits?**
	- In each mapping scheme
		- The block in cache has tag bits
		- The address sent by the processor has tag bits
		- Both are compared
	- Each block's tag bits also needs to be stored in order to be compared
- **Where are the tag bits stored?**
	- These tag bits are also stored in the cache
	- Therefore: the cache has two parts
		- One part to store data, the frame number: data cache
		- One part to store the tags
- **What is the total size of the cache memory?**
	- The total size of the cache memory will be sum of 
		- Size of the data cache
		- Size required to store the tag bits for each block

#### 3 Accessing performance
- CPU performance
	- proportional to number of instructions executed per second
	- When more data/instructions are in cache, the CPU idle time is less -> more instruction can be executed
- Performance metrics
	- Hit rate and Miss rate
		- Hit rate = no of hits/ total accesses
		- Miss rate = no of miss/ total accesses
	- Miss penalty
		- CPU idle time when miss occurs
		- Time required to fetch a block
			- This does not include the first access to cache to check if it is a hit or miss
	- Average access time 
		- t<sub>avg</sub> = (cache access time) + (miss penalty)\*(miss rate)
			- A cache access has to be made to determine if there is a miss or hit
			- The expanded formula is 
				- t<sub>avg</sub> = (cache access time)\*(hit rate)+(cache access time)\*(miss rate) + (miss penalty)\*(miss rate)
				- Miss penalty will be equal to the **main memory access times**
				- Miss penalty does not include the initial cache access time to determine if it is a hit or a miss
		- t<sub>avg</sub> = (cache access time)\*(hit rate) + (page fault service time)\*(miss rate)
			- In this case the page fault service time includes the initial cache access time

#### 4 Replacement algorithms
- What is it?
	- When the cache is full, a block from cache needs to be freed
	- The replacement algorithm chooses the block to replace
- Algos
	- Least recently used
		- Replace the least recently used block from the cache
	- Oldest block
		- replace the oldest block from cache
	- Random
	- Least frequently used

#### 5 Multi level Cache
- In multi level cache there are multiple caches between main mem and processor
	- The cache that is closer to processor is smaller and faster
	- The subsequent levels are bigger and slower
- Access methods #TODO 
	- Hierarchical access
	- Simultaneous access

